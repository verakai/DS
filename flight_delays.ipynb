{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flight_delays.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verakai/DS/blob/master/flight_delays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fqdZNqv46LS",
        "colab_type": "text"
      },
      "source": [
        "# **Big Data with Spark in Google Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Syk1cKh_nsb",
        "colab_type": "text"
      },
      "source": [
        "## Spark and Colaboratory setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTsyMSUy17fM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "d5796ded-2097-4e92-d17e-a11629948871"
      },
      "source": [
        "# Install spark-related depdencies for Python\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
            "\u001b[K     |████████████████████████████████| 215.6MB 98kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzb8TQXf2DWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzUr-r3Y2F6r",
        "colab_type": "code",
        "outputId": "20fbc0b6-9a7c-4107-82b2-1884811da64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Wjzwcc_swK",
        "colab_type": "text"
      },
      "source": [
        "## Data download to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wovKYYQM4vMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download datasets directly to your Google Drive \"Colab Datasets\" folder\n",
        "\n",
        "import requests\n",
        "\n",
        "# 2007 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)\n",
        "\n",
        "# 2008 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMnnHpLcDZKw",
        "colab_type": "text"
      },
      "source": [
        "##  **Import** tools from PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgTbo49l2H_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tools we need to connect to the Spark server, load our data, clean it, and prepare, execute, and evaluate a model\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnY4vcKuDDvo",
        "colab_type": "text"
      },
      "source": [
        "## Set Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypM4p5ti9i-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CSV_2007= \"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\" \n",
        "CSV_2008= \"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\"\n",
        "APP_NAME = \"Flight Delays\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141109\n",
        "TRAINING_DATA_RATIO = 0.7\n",
        "RF_NUM_TREES = 8\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0FRbSOUDMRJ",
        "colab_type": "text"
      },
      "source": [
        "## Connect to the server and load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5AlE_w-3jUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect to the Spark server\n",
        "\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
        "\n",
        "# Load datasets\n",
        "\n",
        "df_2007 = spark.read.options(header=\"true\",inferschema = \"true\").csv(CSV_2007)\n",
        "df_2008 = spark.read.options(header=\"true\",inferschema = \"true\").csv(CSV_2008)\n",
        "\n",
        "# We concatenate both datasets\n",
        "\n",
        "df = df_2007.unionAll(df_2008)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMND_BX1ewlB",
        "colab_type": "text"
      },
      "source": [
        "## Prepare, clean and validate the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcuffbHxe1ae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45c64597-81ec-4970-e2a4-87976aaa52b9"
      },
      "source": [
        "# What's the data shape before starting cleaning ?\n",
        "\n",
        "print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape is 14462943 rows by 29 columns.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6wY9rQjgU5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9db2ae60-a864-40e6-a506-7e99431b5e43"
      },
      "source": [
        "# What's the number of null values ?\n",
        "\n",
        "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
        "                         for c in df.columns]).toPandas().to_dict(orient='records')\n",
        "\n",
        "print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 14248147 null values in this dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAOe1qE-gfOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop null columns and inputs ?\n",
        "\n",
        "df = df.drop(df.CancellationCode)\n",
        "df = df.na.drop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2rnFflugfSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3667d0b0-fb34-4746-a25a-da93c8fa67ee"
      },
      "source": [
        "# Confirm there are no null values\n",
        "\n",
        "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
        "                         for c in df.columns]).toPandas().to_dict(orient='records')\n",
        "\n",
        "print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 0 null values in this dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3oIOyVSWQb2",
        "colab_type": "code",
        "outputId": "08778c71-6df8-4503-a2e7-41e5be5d6a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# What's the data shape after cleaning ?\n",
        "\n",
        "print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape is 14379556 rows by 28 columns.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQyE4UJJI1i8",
        "colab_type": "text"
      },
      "source": [
        "## Set up and run our classifier in Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e98yIqdmKWcK",
        "colab_type": "code",
        "outputId": "5161e94d-f05f-45c0-f25c-2b30c32afe6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# What are the column's type ?\n",
        "\n",
        "df.dtypes"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Year', 'int'),\n",
              " ('Month', 'int'),\n",
              " ('DayofMonth', 'int'),\n",
              " ('DayOfWeek', 'int'),\n",
              " ('DepTime', 'string'),\n",
              " ('CRSDepTime', 'int'),\n",
              " ('ArrTime', 'string'),\n",
              " ('CRSArrTime', 'int'),\n",
              " ('UniqueCarrier', 'string'),\n",
              " ('FlightNum', 'int'),\n",
              " ('TailNum', 'string'),\n",
              " ('ActualElapsedTime', 'string'),\n",
              " ('CRSElapsedTime', 'string'),\n",
              " ('AirTime', 'string'),\n",
              " ('ArrDelay', 'string'),\n",
              " ('DepDelay', 'string'),\n",
              " ('Origin', 'string'),\n",
              " ('Dest', 'string'),\n",
              " ('Distance', 'int'),\n",
              " ('TaxiIn', 'string'),\n",
              " ('TaxiOut', 'string'),\n",
              " ('Cancelled', 'int'),\n",
              " ('Diverted', 'int'),\n",
              " ('CarrierDelay', 'string'),\n",
              " ('WeatherDelay', 'string'),\n",
              " ('NASDelay', 'string'),\n",
              " ('SecurityDelay', 'string'),\n",
              " ('LateAircraftDelay', 'string')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPJdcVZhI4Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create list of feature columns\n",
        "\n",
        "feature_cols = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', \n",
        "                'CRSArrTime', 'FlightNum', 'Distance', 'Diverted']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4ZETOBtKKlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate and create our new feature vector column\n",
        "\n",
        "df = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgWPhJ9XK3Fo",
        "colab_type": "code",
        "outputId": "3313ee9d-d191-4eb7-f415-796a90f3303b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Select input columns\n",
        "\n",
        "df.select(\"Cancelled\", \"features\").show(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------------------+\n",
            "|Cancelled|            features|\n",
            "+---------+--------------------+\n",
            "|        0|[2007.0,1.0,1.0,1...|\n",
            "|        0|[2007.0,1.0,1.0,1...|\n",
            "|        0|[2007.0,1.0,1.0,1...|\n",
            "|        0|[2007.0,1.0,1.0,1...|\n",
            "|        0|[2007.0,1.0,1.0,1...|\n",
            "+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZrvTdxBLA2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the training indexers\n",
        "\n",
        "# Generate a labelIndexer\n",
        "labelIndexer = StringIndexer(inputCol=\"Cancelled\", outputCol=\"indexedLabel\").fit(df)\n",
        "\n",
        "# Generate the indexed feature vector\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
        "    \n",
        "# Split the data into training and tests sets\n",
        "(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
        "\n",
        "# Train the RandomForest model\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
        "\n",
        "# Chain indexers and the forest models in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjE_avVbLfmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model = pipeline.fit(trainingData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SderkFFTSNw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions\n",
        "\n",
        "predictions = model.transform(testData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSOdyvSpLl9e",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTGwK2jkLlKJ",
        "colab_type": "code",
        "outputId": "fcb7ca99-7db0-44e5-fe70-1bcbed707638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Select prediction, true label and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
        "print(f\"Accuracy = {accuracy:g}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.0149426\n",
            "Accuracy = 0.985057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtaUvH7XSFpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}